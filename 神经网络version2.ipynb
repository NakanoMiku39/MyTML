{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3c90c6",
   "metadata": {},
   "source": [
    "# 将RGBA图片修改为RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b4e9958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test23.jpg\n",
      "test24.jpg\n",
      "test25.jpg\n",
      "test26.jpg\n",
      "test27.jpg\n",
      "test29.jpg\n",
      "test30.jpg\n",
      "test31.jpg\n",
      "test32.jpg\n",
      "test33.jpg\n",
      "test38.png\n",
      "test39.png\n",
      "test40.png\n",
      "test41.png\n",
      "test42.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "'''png格式常常是32位的RGBA格式，A代表透明度，\n",
    "   光是更改图片后缀，不能改变图片的位数，\n",
    "   需要在openCV中进行色彩空间的转换，\n",
    "   将png格式的32位RGBA转为jpg格式的24位RGB'''\n",
    "\n",
    "def convert2jpg(filename):                                      # 将彩色图转灰度图的函数\n",
    "    img = cv2.imread(file_path+'/'+filename, 1)                 # 1是以彩色图方式去读\n",
    "    jpg_img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    cv2.imwrite(out_path + '/' + filename, jpg_img)             # 保存在新文件夹下，且图名中加GRAY\n",
    "\n",
    "file_path = \"./edit\"                         # 输入文件夹\n",
    "#os.mkdir(\"./edit\")                           # 建立新的目录\n",
    "out_path =\"./attack\"                           # 设置为新目录为输出文件夹\n",
    "\n",
    "for filename in os.listdir(file_path):                          # 遍历输入路径，得到图片名\n",
    "    print(filename)\n",
    "    convert2jpg(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae08fe",
   "metadata": {},
   "source": [
    "# 初始化数据集和网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "947e85d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIVE\n",
      "DATA SET\n",
      "DATA SET\n",
      "DATA SET\n",
      "DATA SET\n",
      "MODEL SET\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd #用于更轻松的csv解析\n",
    "from PIL import Image\n",
    "import random\n",
    "from scipy.optimize import differential_evolution\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "print(\"ALIVE\")\n",
    "\n",
    "characters = [\"marin\", \"miku\", \"kaguya\"]\n",
    "\n",
    "#数据集\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, root_dir, csvfile, transform=transforms.ToTensor()):\n",
    "        self.root_dir = root_dir#图像所在目录\n",
    "        self.csv = pd.read_csv(csvfile)#标记所在目录\n",
    "        print(\"DATA SET\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "           \n",
    "        #图片路径\n",
    "        img_path = os.path.join(self.root_dir, self.csv.iloc[idx, 0])#单个图像路径    \n",
    "        image_transform = transforms.Compose([\n",
    "        # 将输入图片resize成统一尺寸\n",
    "        transforms.Resize([128, 128]),\n",
    "        # 将PIL Image或numpy.ndarray转换为tensor，并除255归一化到[0,1]之间\n",
    "        transforms.ToTensor(),\n",
    "        # 标准化处理-->转换为标准正太分布，使模型更容易收敛\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "        image = Image.open(img_path)#打开图像\n",
    "        image = image_transform(image)\n",
    "        label = self.csv.iloc[idx, 1]#打开图像对应标签\n",
    "        label = np.array(label)#标签矩阵化\n",
    "        return image, label, #返回图像和标签\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv)#返回图片个数\n",
    "    \n",
    "#没有标准化处理的数据集，用作攻击\n",
    "class MyData2(Dataset):\n",
    "    def __init__(self, root_dir, csvfile, transform=transforms.ToTensor()):\n",
    "        self.root_dir = root_dir#图像所在目录\n",
    "        self.csv = pd.read_csv(csvfile)#标记所在目录\n",
    "        print(\"DATA SET\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "           \n",
    "        #图片路径\n",
    "        img_path = os.path.join(self.root_dir, self.csv.iloc[idx, 0])#单个图像路径\n",
    "        \n",
    "        image_transform = transforms.Compose([\n",
    "        # 将输入图片resize成统一尺寸\n",
    "        transforms.Resize([128, 128]),\n",
    "        # 将PIL Image或numpy.ndarray转换为tensor，并除255归一化到[0,1]之间\n",
    "        transforms.ToTensor(),\n",
    "        # 标准化处理-->转换为标准正太分布，使模型更容易收敛\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        image = Image.open(img_path)#打开图像\n",
    "        image = image_transform(image)\n",
    "        label = self.csv.iloc[idx, 1]#打开图像对应标签\n",
    "        label = np.array(label)#标签矩阵化\n",
    "        return image, label, #返回图像和标签\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv)#返回图片个数\n",
    "    \n",
    "    \n",
    "#数据集对象实例化\n",
    "mydata = MyData(root_dir = \"./after3\", csvfile = \"./after3/after3.csv\")#训练数据\n",
    "trainloader = torch.utils.data.DataLoader(mydata, batch_size = 100, shuffle = True)\n",
    "testdata = MyData(root_dir = \"./try\", csvfile = \"./try/try.csv\")#测试数据\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size = 1, shuffle = True)\n",
    "attackdata = MyData2(root_dir = \"./attack\", csvfile = \"./attack/attack.csv\")#攻击数据\n",
    "attackloader = torch.utils.data.DataLoader(attackdata, shuffle = False)\n",
    "afterdata = MyData2(root_dir = \"./train\", csvfile = \"./train/train.csv\")#扰动后数据\n",
    "afterloader = torch.utils.data.DataLoader(afterdata, batch_size = 1, shuffle = False)\n",
    "\n",
    "#神经网络\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)#第一卷积层\n",
    "        self.conv2 = nn.Conv2d(64, 1, 5)#第二卷积层\n",
    "        self.fc1 = nn.Linear(1600, 256)#第一全连接层\n",
    "        self.fc2 = nn.Linear(256, 3)\n",
    "        self.pool = nn.MaxPool2d(3, 3)#池化层\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))#第一层卷积后relu\n",
    "        x = self.pool(F.relu(self.conv2(x)))#第二层卷积后relu且池化\n",
    "        x = x.view(x.shape[0], -1)#将图片降维\n",
    "        x = F.relu(self.fc1(x))#带入第一全连接层\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "#神经网络对象实例化\n",
    "print(\"MODEL SET\")\n",
    "net = MyNet()\n",
    "\n",
    "#模型保存路径\n",
    "PATH = \".\\myModel-2.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b158d",
   "metadata": {},
   "source": [
    "# 训练神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac62f27c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!!!\n",
      "\n",
      "-->EPOCH: 0\n",
      "Loss 64.9605702161789\n",
      "\n",
      "-->EPOCH: 1\n",
      "Loss 22.586461178958416\n",
      "\n",
      "-->EPOCH: 2\n",
      "Loss 6.7054756712168455\n",
      "\n",
      "-->EPOCH: 3\n",
      "Loss 4.084041508845985\n",
      "\n",
      "-->EPOCH: 4\n",
      "Loss 2.286012116353959\n",
      "\n",
      "-->EPOCH: 5\n",
      "Loss 1.2317344893235713\n",
      "\n",
      "-->EPOCH: 6\n",
      "Loss 0.8550743779633194\n",
      "\n",
      "-->EPOCH: 7\n",
      "Loss 0.6080471783643588\n",
      "\n",
      "-->EPOCH: 8\n",
      "Loss 0.6259400200797245\n",
      "\n",
      "-->EPOCH: 9\n",
      "Loss 0.3662434268044308\n",
      "\n",
      "-->EPOCH: 10\n",
      "Loss 0.28251621779054403\n",
      "\n",
      "-->EPOCH: 11\n",
      "Loss 0.22824398119701073\n",
      "\n",
      "-->EPOCH: 12\n",
      "Loss 0.1935069459141232\n",
      "\n",
      "-->EPOCH: 13\n",
      "Loss 0.3528742193011567\n",
      "\n",
      "-->EPOCH: 14\n",
      "Loss 0.22475842700805515\n",
      "\n",
      "Process done...\n",
      "Successfully Saved...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#定义损失和优化\n",
    "MyLoss = nn.CrossEntropyLoss()#交叉熵\n",
    "MyOptim = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)#更新权重使用SGD更新规则\n",
    "\n",
    "\n",
    "print(\"Start training!!!\")\n",
    "\n",
    "for epoch in range(15):#训练十次防止过拟合\n",
    "    runningLoss = 0\n",
    "    print(\"\\n-->EPOCH:\", epoch)\n",
    "    for i, data in enumerate(trainloader, 0):#遍历训练数据\n",
    "        \n",
    "        inputs, labels = data#带入训练数据\n",
    "        \n",
    "        MyOptim.zero_grad()#清零梯度\n",
    "        \n",
    "        #print(inputs.shape)\n",
    "        outputs = net(inputs.to(torch.float32))#带入神经网络\n",
    "        \n",
    "        \n",
    "        loss = MyLoss(outputs, labels)#计算损失\n",
    "        \n",
    "        \n",
    "        loss.backward()#反向传播\n",
    "        \n",
    "       \n",
    "        MyOptim.step()#更改权重\n",
    "        \n",
    "        runningLoss += loss.item()\n",
    "        \n",
    "    print(\"Loss\", runningLoss)\n",
    "    runningLoss = 0\n",
    "        \n",
    "  \n",
    "print(\"\\nProcess done...\")\n",
    "\n",
    "\n",
    "#保存神经网络模型\n",
    "\n",
    "torch.save(net.state_dict(), PATH)\n",
    "print(\"Successfully Saved...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ef829",
   "metadata": {},
   "source": [
    "# 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9afbf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的是testloader数据集\n",
      "共计： 120 正确： 91 准确率： 0.7583333333333333\n",
      "marin出现次数: 45 正确： 44 准确率： 0.9777777777777777\n",
      "miku出现次数： 35 正确： 19 准确率： 0.5428571428571428\n",
      "kaguya出现次数: 40 正确： 28 准确率： 0.7\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(PATH))\n",
    "net.eval()\n",
    "\n",
    "tc = [0,0,0]#统计各个人物出现的次数\n",
    "c = [0,0,0]#统计各个人物识别正确的次数\n",
    "#开始测试数据\n",
    "total_correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader):\n",
    "        images, labels = data\n",
    "        outputs = net(images.to(torch.float32))\n",
    "        name, predicts = torch.max(outputs.data, 1)\n",
    "        if predicts == labels:\n",
    "            total_correct += 1\n",
    "            tc[labels] += 1\n",
    "        total += 1\n",
    "        c[labels] += 1\n",
    "print(\"当前使用的是testloader数据集\")\n",
    "print(\"共计：\", total, \"正确：\", total_correct,\"准确率：\", total_correct / total)\n",
    "print(\"marin出现次数:\", c[0], \"正确：\", tc[0],\"准确率：\", tc[0] / c[0])\n",
    "print(\"miku出现次数：\", c[1], \"正确：\", tc[1],\"准确率：\", tc[1] / c[1])\n",
    "print(\"kaguya出现次数:\", c[2], \"正确：\", tc[2],\"准确率：\", tc[2] / c[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037a7ef",
   "metadata": {},
   "source": [
    "# FGSM攻击网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d7cca72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinle\\AppData\\Local\\Temp\\ipykernel_17440\\2448173834.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ori = torch.tensor(ori)#转为tensor\n",
      "C:\\Users\\jinle\\AppData\\Local\\Temp\\ipykernel_17440\\2448173834.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 20 / 30 = 0.6666666666666666\n",
      "\n",
      "Epsilon: 0.01\tTest Accuracy = 16 / 30 = 0.5333333333333333\n",
      "\n",
      "Epsilon: 0.015\tTest Accuracy = 14 / 30 = 0.4666666666666667\n",
      "\n",
      "Epsilon: 0.02\tTest Accuracy = 11 / 30 = 0.36666666666666664\n",
      "\n",
      "Epsilon: 0.025\tTest Accuracy = 11 / 30 = 0.36666666666666664\n",
      "\n",
      "Epsilon: 0.03\tTest Accuracy = 11 / 30 = 0.36666666666666664\n",
      "\n",
      "Epsilon: 0.035\tTest Accuracy = 11 / 30 = 0.36666666666666664\n",
      "\n",
      "Epsilon: 0.04\tTest Accuracy = 11 / 30 = 0.36666666666666664\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(PATH))\n",
    "net.eval()\n",
    "\n",
    "\n",
    "#扰动\n",
    "epsilons = [0, .01, .015, .02, .025, .03, .035, .04]\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # 取图片张量每个元素的梯度\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # 在每个像素上添加扰动，幅度取决于epsilon\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # 将图片张量的每个元素的取值控制在0和1之间\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # 返回扰动图片\n",
    "    return perturbed_image\n",
    "\n",
    "def test( model, test_loader, epsilon):\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    examples = []\n",
    "    diffs = []\n",
    "    \n",
    "    #迭代梯度\n",
    "    for data, labels in test_loader:   \n",
    "        \n",
    "        inputs, labels = data, labels\n",
    "       \n",
    "        inputs.requires_grad = True\n",
    "        \n",
    "        output=net(inputs)#带入图片\n",
    "\n",
    "        init_pred = output.max(1, keepdim=True)[1] #开始预测\n",
    "\n",
    "        if init_pred.item() != labels.item():#如果预测错误的话则是本来就分类错的样本\n",
    "                continue#如果预测正确就是要进行扰动的样本\n",
    "\n",
    "        loss = F.nll_loss(output,labels)#计算损失\n",
    "\n",
    "        net.zero_grad()#清零模型梯度\n",
    "\n",
    "        loss.backward()#反向传播\n",
    "\n",
    "        data_grad = inputs.grad.data#获取反向传播的梯度\n",
    "\n",
    "        perturbed_inputs = fgsm_attack(inputs, epsilon, data_grad)#制作扰动图像\n",
    "\n",
    "        output = net(perturbed_inputs)#带入识别扰动图像\n",
    "\n",
    "        final_pred = output.max(1, keepdim=True)[1]#看扰动后图像的标签\n",
    "        \n",
    "        if final_pred.item() == labels.item():#如果添加扰动后依然识别成功\n",
    "            correct += 1\n",
    "        \n",
    "        #原本的图片\n",
    "        ori = inputs[0]\n",
    "        ori = torch.tensor(ori)#转为tensor\n",
    "        ori = ori.detach().numpy()#去除梯度\n",
    "        ori = np.transpose(ori, (1,2,0))#转置\n",
    "\n",
    "        #加了扰动后的图片\n",
    "        img = perturbed_inputs[0] # plt.imshow()只能接受3-d tensor，所以也要用image[0]消去batch那一维\n",
    "        img = torch.tensor(img)\n",
    "        img = img.detach().numpy() # floattensor转为ndarray\n",
    "        img = np.transpose(img, (1,2,0)) # 把channel那一维放到最后\n",
    "        \n",
    "        diff = 255*(ori-img)\n",
    "\n",
    "        examples.append(img)\n",
    "        diffs.append(diff)\n",
    "        \n",
    "    '''\n",
    "    plt.figure(figsize=(30,30))\n",
    "    \n",
    "    for i in range(10):\n",
    "        ax1 = plt.subplot(1, 10, i + 1) \n",
    "        ax1.axis(\"off\")       \n",
    "        ax1.imshow(examples[i])\n",
    "    \n",
    "    '''\n",
    "    plt.show()\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\\n\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc= test(net, attackloader, eps)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "\n",
    "plt.plot(epsilons, accuracies)\n",
    "plt.xlabel(\"epsilons\")\n",
    "plt.ylabel(\"accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32712c",
   "metadata": {},
   "source": [
    "# 单像素攻击 One Pixel Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bd84145",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinle\\AppData\\Local\\Temp\\ipykernel_17440\\1338765959.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs)[0]\n",
      "C:\\Users\\jinle\\.conda\\envs\\pytorchWithPython3.8\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "C:\\Users\\jinle\\.conda\\envs\\pytorchWithPython3.8\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的是attackloader数据集\n",
      "correct:13, total:20, acc:0.650000\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(PATH))\n",
    "net.eval()\n",
    "\n",
    "def perturb_image(xs, img):#对图片进行单像素更改\n",
    "    # If this function is passed just one perturbation vector,\n",
    "    # pack it in a list to keep the computation the same\n",
    "    if xs.ndim < 2:\n",
    "        xs = np.array([xs])\n",
    "    \n",
    "    # Copy the image n == len(xs) times so that we can \n",
    "    # create n new perturbed images\n",
    "    tile = [len(xs)] + [1]*(xs.ndim+1)\n",
    "    imgs = np.tile(img, tile)\n",
    "    \n",
    "    # Make sure to floor the members of xs as int types\n",
    "    xs = xs.astype(int)\n",
    "    \n",
    "    for x,img in zip(xs, imgs):\n",
    "        # Split x into an array of 5-tuples (perturbation pixels)\n",
    "        # i.e., [[x,y,r,g,b], ...]\n",
    "        pixels = np.split(x, len(x) // 5)\n",
    "        for pixel in pixels:\n",
    "            # At each pixel's x,y position, assign its rgb value\n",
    "            x_pos, y_pos, *rgb = pixel\n",
    "            img[x_pos, y_pos] = rgb\n",
    "    \n",
    "    return torch.tensor(np.transpose(imgs, (0,3,2,1)))\n",
    "\n",
    "def predict_classes(xs, img, model, minimize=True):#预测图片类型\n",
    "    # Perturb the image with the given pixel(s) x and get the prediction of the model\n",
    "    imgs_perturbed = perturb_image(xs, img)#对图片进行单像素更改\n",
    "    predictions = model(imgs_perturbed).max(1, keepdim=True)[1]#带入模型后查看预测结果\n",
    "    # This function should always be minimized, so return its complement if needed\n",
    "    return predictions if minimize else 1 - predictions\n",
    "\n",
    "def attack_success(x, img, target_class, model):\n",
    "    # Perturb the image with the given pixel(s) and get the prediction of the model\n",
    "    attack_image = perturb_image(x, img)\n",
    "    prediction = net(attack_image).max(1, keepdim=True)\n",
    "    # If the prediction is what we want (misclassification or \n",
    "    # targeted classification), return True\n",
    "    if (prediction != target_class):\n",
    "         return True\n",
    "    # NOTE: return None otherwise (not False), due to how Scipy handles its callback function\n",
    "\n",
    "def opa(pixel_count = 1, maxiter = 100, popsize = 500):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    examples = []\n",
    "    \n",
    "    bounds = [(0,128), (0,128), (0,256), (0,256), (0,256)] * pixel_count\n",
    "    # Population multiplier, in terms of the size of the perturbation vector x\n",
    "    popmul = max(1, popsize // len(bounds))\n",
    "    \n",
    "    for i, data in enumerate(attackloader):#遍历训练数据\n",
    "        #print(\"start progress\")\n",
    "        inputs, labels = data#带入训练数据\n",
    "        out = net(inputs).max(1, keepdim = True)[1]\n",
    "        if(out != labels):\n",
    "            continue\n",
    "    \n",
    "        inputs = torch.tensor(inputs)[0]\n",
    "        inputs = np.transpose(inputs, (1,2,0))        \n",
    "        test = copy.deepcopy(inputs)\n",
    "        \n",
    "        def predict_fn(xs):\n",
    "            return predict_classes(xs, test, net)#返回预测结果\n",
    "    \n",
    "        def callback_fn(x, convergence):\n",
    "            return attack_success(x, test, labels, \n",
    "                              net)\n",
    "    \n",
    "        # Call Scipy's Implementation of Differential Evolution\n",
    "        attack_result = differential_evolution(\n",
    "        predict_fn, bounds, maxiter=maxiter, popsize=popmul,\n",
    "        recombination=1, atol=-1, callback=callback_fn, polish=False)\n",
    "        \n",
    "        \n",
    "        #print(\"x:%d, y:%d, r:%d, g:%d, b:%d\" %(attack_result.x[0], attack_result.x[1], attack_result.x[2], attack_result.x[3], attack_result.x[4]))\n",
    "        perturbed =  perturb_image(attack_result.x, inputs)#对图片进行单像素攻击\n",
    "            \n",
    "        outputs = net(perturbed)#带入神经网络\n",
    "\n",
    "        predict = outputs.max(1, keepdim=True)[1]\n",
    "        perturbed = np.transpose(perturbed[0], (2,1,0))\n",
    "        #print(\"Predict: %s, Actual: %s\\n\" % (characters[predict], characters[labels]))\n",
    "        #plt.imshow(((perturbed.detach().numpy())*255).astype(np.uint8))\n",
    "        plt.show()\n",
    "        if predict == labels:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        examples.append(inputs)\n",
    "        \n",
    "    '''\n",
    "    plt.figure(figsize=(50,50))\n",
    "    \n",
    "    for i in range(3):\n",
    "        ax1 = plt.subplot(1, 3, i + 1) \n",
    "        ax1.axis(\"off\")       \n",
    "        ax1.imshow(((examples[i].detach().numpy())*255).astype(np.uint8))\n",
    "    \n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "    return correct, total\n",
    "\n",
    "c, t = opa()\n",
    "print(\"当前使用的是attackloader数据集\")\n",
    "print(\"correct:%d, total:%d, acc:%f\" % (c, t, c/t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be226fd4",
   "metadata": {},
   "source": [
    "# 生成FGSM对抗数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee09fb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinle\\AppData\\Local\\Temp\\ipykernel_39268\\2809804108.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(PATH))\n",
    "net.eval()\n",
    "\n",
    "#扰动\n",
    "epsilons = [0, .01, .02, .03]\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # 取图片张量每个元素的梯度\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # 在每个像素上添加扰动，幅度取决于epsilon\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # 将图片张量的每个元素的取值控制在0和1之间\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # 返回扰动图片\n",
    "    return perturbed_image\n",
    "\n",
    "def test( model, test_loader, epsilon, i, csv_writer):\n",
    "    j = 1\n",
    "    \n",
    "    #迭代梯度\n",
    "    for data, labels in test_loader:   \n",
    "        \n",
    "        inputs, labels = data, labels\n",
    "       \n",
    "        inputs.requires_grad = True\n",
    "        \n",
    "        output=net(inputs)#带入图片\n",
    "\n",
    "        loss = F.nll_loss(output,labels)#计算损失\n",
    "\n",
    "        net.zero_grad()#清零模型梯度\n",
    "\n",
    "        loss.backward()#反向传播\n",
    "\n",
    "        data_grad = inputs.grad.data#获取反向传播的梯度\n",
    "\n",
    "        perturbed_inputs = fgsm_attack(inputs, epsilon, data_grad)#制作扰动图像\n",
    "\n",
    "        #加了扰动后的图片\n",
    "        img = perturbed_inputs[0] # plt.imshow()只能接受3-d tensor，所以也要用image[0]消去batch那一维\n",
    "        img = torch.tensor(img)\n",
    "        img = img.detach().numpy() # floattensor转为ndarray\n",
    "        img = np.transpose(img, (1,2,0)) # 把channel那一维放到最后\n",
    "\n",
    "        save_name = \"%s%d-%d.jpg\" % (\"after2-\", i, j)\n",
    "        save_path = \"./after2/%s\" % (save_name)\n",
    "        Image.fromarray(np.uint8(img*255)).convert('RGB').save(save_path)\n",
    "        j += 1\n",
    "        csv_writer.writerow([save_name, labels.item()])\n",
    "\n",
    "\n",
    "i = 1\n",
    "# Run test for each epsilon\n",
    "csv_path = \"./after2/after2.csv\"\n",
    "file = open(csv_path, 'a+', encoding='utf-8', newline='')\n",
    "csv_writer = csv.writer(file)\n",
    "csv_writer.writerow([\"character\", \"name\"])\n",
    "for eps in epsilons:\n",
    "    test(net, afterloader, eps, i, csv_writer)\n",
    "    i += 1\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248d45f",
   "metadata": {},
   "source": [
    "# PGD攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b8b04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 14 / 30 = 0.4666666666666667\n",
      "\n",
      "Epsilon: 0.01\tTest Accuracy = 3 / 30 = 0.1\n",
      "\n",
      "Epsilon: 0.015\tTest Accuracy = 1 / 30 = 0.03333333333333333\n",
      "\n",
      "Epsilon: 0.02\tTest Accuracy = 0 / 30 = 0.0\n",
      "\n",
      "Epsilon: 0.025\tTest Accuracy = 0 / 30 = 0.0\n",
      "\n",
      "Epsilon: 0.03\tTest Accuracy = 0 / 30 = 0.0\n",
      "\n",
      "Epsilon: 0.035\tTest Accuracy = 0 / 30 = 0.0\n",
      "\n",
      "Epsilon: 0.04\tTest Accuracy = 0 / 30 = 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(PATH))\n",
    "net.eval()\n",
    "\n",
    "# 扰动\n",
    "epsilons = [0, .01, .015, .02, .025, .03, .035, .04]\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "class PGD(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model # 必须是pytorch的model\n",
    "        self.device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "    def generate(self, x, **params):\n",
    "        self.parse_params(**params)\n",
    "        labels = self.y\n",
    "        adv_x = self.attack(x, labels)\n",
    "        return adv_x\n",
    "\n",
    "    def parse_params(self, eps=0.3, iter_eps=0.001, nb_iter=100, clip_min=0.0, clip_max=1.0, C=0.0,\n",
    "                     y=None, ord=np.inf, rand_init=True, flag_target=False):\n",
    "        self.eps = eps\n",
    "        self.iter_eps = iter_eps\n",
    "        self.nb_iter = nb_iter\n",
    "        self.clip_min = clip_min\n",
    "        self.clip_max = clip_max\n",
    "        self.y = y\n",
    "        self.ord = ord\n",
    "        self.rand_init = rand_init\n",
    "        self.model.to(self.device)\n",
    "        self.flag_target = flag_target\n",
    "        self.C = C\n",
    "\n",
    "    def sigle_step_attack(self, x, pertubation, labels):\n",
    "        adv_x = x + pertubation\n",
    "        # get the gradient of x\n",
    "        # get the gradient of x\n",
    "        adv_x = Variable(adv_x)\n",
    "        adv_x.requires_grad = True\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        preds = self.model(adv_x)\n",
    "        if self.flag_target:\n",
    "            loss = -loss_func(preds, labels)\n",
    "        else:\n",
    "            loss = loss_func(preds, labels)\n",
    "            # label_mask=torch_one_hot(labels)\n",
    "            #\n",
    "            # correct_logit=torch.mean(torch.sum(label_mask * preds,dim=1))\n",
    "            # wrong_logit = torch.mean(torch.max((1 - label_mask) * preds, dim=1)[0])\n",
    "            # loss=-F.relu(correct_logit-wrong_logit+self.C)\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = adv_x.grad.data\n",
    "        # get the pertubation of an iter_eps\n",
    "        pertubation = self.iter_eps * np.sign(grad)\n",
    "        adv_x = adv_x.cpu().detach().numpy() + pertubation.cpu().numpy()\n",
    "        x = x.cpu().detach().numpy()\n",
    "        pertubation = np.clip(adv_x, self.clip_min, self.clip_max) - x\n",
    "        pertubation = np.clip(pertubation, -self.eps, self.eps)\n",
    "        return pertubation\n",
    "\n",
    "    def attack(self, x, labels):\n",
    "        labels = labels.to(self.device)\n",
    "        #print(self.rand_init)\n",
    "        if self.rand_init:\n",
    "            x_tmp = x + torch.Tensor(np.random.uniform(-self.eps, self.eps, x.shape)).type_as(x)\n",
    "        else:\n",
    "            x_tmp = x\n",
    "        pertubation = torch.zeros(x.shape).type_as(x).to(self.device)\n",
    "        for i in range(self.nb_iter):\n",
    "            pertubation = self.sigle_step_attack(x_tmp, pertubation=pertubation, labels=labels)\n",
    "            pertubation = torch.Tensor(pertubation).type_as(x).to(self.device)\n",
    "        adv_x = x + pertubation\n",
    "        adv_x = np.clip(adv_x, self.clip_min, self.clip_max)\n",
    "        return adv_x\n",
    "\n",
    "pgd = PGD(net)\n",
    "\n",
    "def test(model, test_loader, epsilon):\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    examples = []\n",
    "    diffs = []\n",
    "\n",
    "    # 迭代梯度\n",
    "    for data, labels in test_loader:\n",
    "\n",
    "        inputs, labels = data, labels\n",
    "\n",
    "        output = net(inputs)  # 带入图片\n",
    "\n",
    "        init_pred = output.max(1, keepdim=True)[1]  # 开始预测\n",
    "\n",
    "        if init_pred.item() != labels.item():  # 如果预测错误的话则是本来就分类错的样本\n",
    "            continue  # 如果预测正确就是要进行扰动的样本\n",
    "\n",
    "        pgd.parse_params(eps=epsilon)\n",
    "\n",
    "        perturbed_inputs = pgd.attack(inputs, labels)  # 制作扰动图像\n",
    "\n",
    "        output = net(perturbed_inputs)  # 带入识别扰动图像\n",
    "\n",
    "        final_pred = output.max(1, keepdim=True)[1]  # 看扰动后图像的标签\n",
    "\n",
    "        if final_pred.item() == labels.item():  # 如果添加扰动后依然识别成功\n",
    "            correct += 1\n",
    "\n",
    "        # 原本的图片\n",
    "        ori = inputs[0]\n",
    "        ori = torch.as_tensor(ori)  # 转为tensor\n",
    "        ori = ori.detach().numpy()  # 去除梯度\n",
    "        ori = np.transpose(ori, (1, 2, 0))  # 转置\n",
    "\n",
    "        # 加了扰动后的图片\n",
    "        img = perturbed_inputs[0]  # plt.imshow()只能接受3-d tensor，所以也要用image[0]消去batch那一维\n",
    "        img = torch.as_tensor(img)\n",
    "        img = img.detach().numpy()  # floattensor转为ndarray\n",
    "        img = np.transpose(img, (1, 2, 0))  # 把channel那一维放到最后\n",
    "\n",
    "        diff = 255 * (ori - img)\n",
    "\n",
    "        examples.append(img)\n",
    "        diffs.append(diff)\n",
    "\n",
    "    #plt.figure(figsize=(30, 30))\n",
    "\n",
    "    #plt.imshow(examples[1])\n",
    "    #plt.show()\n",
    "    final_acc = correct / float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\\n\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc = test(net, attackloader, eps)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "plt.plot(epsilons, accuracies)\n",
    "plt.xlabel(\"epsilons\")\n",
    "plt.ylabel(\"accuracies\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5fcf975",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8827f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "无",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3c90c6",
   "metadata": {},
   "source": [
    "# 将RGBA图片修改为RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b4e9958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test23.jpg\n",
      "test24.jpg\n",
      "test25.jpg\n",
      "test26.jpg\n",
      "test27.jpg\n",
      "test29.jpg\n",
      "test30.jpg\n",
      "test31.jpg\n",
      "test32.jpg\n",
      "test33.jpg\n",
      "test38.png\n",
      "test39.png\n",
      "test40.png\n",
      "test41.png\n",
      "test42.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "'''png格式常常是32位的RGBA格式，A代表透明度，\n",
    "   光是更改图片后缀，不能改变图片的位数，\n",
    "   需要在openCV中进行色彩空间的转换，\n",
    "   将png格式的32位RGBA转为jpg格式的24位RGB'''\n",
    "\n",
    "def convert2jpg(filename):                                      # 将彩色图转灰度图的函数\n",
    "    img = cv2.imread(file_path+'/'+filename, 1)                 # 1是以彩色图方式去读\n",
    "    jpg_img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    cv2.imwrite(out_path + '/' + filename, jpg_img)             # 保存在新文件夹下，且图名中加GRAY\n",
    "\n",
    "file_path = \"./edit\"                         # 输入文件夹\n",
    "#os.mkdir(\"./edit\")                           # 建立新的目录\n",
    "out_path =\"./attack\"                           # 设置为新目录为输出文件夹\n",
    "\n",
    "for filename in os.listdir(file_path):                          # 遍历输入路径，得到图片名\n",
    "    print(filename)\n",
    "    convert2jpg(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae08fe",
   "metadata": {},
   "source": [
    "# 初始化数据集和网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947e85d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIVE\n",
      "DATA SET\n",
      "DATA SET\n",
      "DATA SET\n",
      "DATA SET\n",
      "MODEL SET\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage import io\n",
    "import pandas as pd #用于更轻松的csv解析\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import csv\n",
    "import cv2\n",
    "\n",
    "print(\"ALIVE\")\n",
    "\n",
    "characters = [\"marin\", \"miku\", \"kaguya\"]\n",
    "\n",
    "#数据集\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, root_dir, csvfile, transform=transforms.ToTensor()):\n",
    "        self.root_dir = root_dir#图像所在目录\n",
    "        self.csv = pd.read_csv(csvfile)#标记所在目录\n",
    "        print(\"DATA SET\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "           \n",
    "        #图片路径\n",
    "        img_path = os.path.join(self.root_dir, self.csv.iloc[idx, 0])#单个图像路径    \n",
    "        image_transform = transforms.Compose([\n",
    "        # 将输入图片resize成统一尺寸\n",
    "        transforms.Resize([128, 128]),\n",
    "        # 将PIL Image或numpy.ndarray转换为tensor，并除255归一化到[0,1]之间\n",
    "        transforms.ToTensor(),\n",
    "        # 标准化处理-->转换为标准正太分布，使模型更容易收敛\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "        image = Image.open(img_path)#打开图像\n",
    "        image = image_transform(image)\n",
    "        label = self.csv.iloc[idx, 1]#打开图像对应标签\n",
    "        label = np.array(label)#标签矩阵化\n",
    "        return image, label, #返回图像和标签\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv)#返回图片个数\n",
    "    \n",
    "#没有标准化处理的数据集，用作攻击\n",
    "class MyData2(Dataset):\n",
    "    def __init__(self, root_dir, csvfile, transform=transforms.ToTensor()):\n",
    "        self.root_dir = root_dir#图像所在目录\n",
    "        self.csv = pd.read_csv(csvfile)#标记所在目录\n",
    "        print(\"DATA SET\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "           \n",
    "        #图片路径\n",
    "        img_path = os.path.join(self.root_dir, self.csv.iloc[idx, 0])#单个图像路径\n",
    "        \n",
    "        image_transform = transforms.Compose([\n",
    "        # 将输入图片resize成统一尺寸\n",
    "        transforms.Resize([128, 128]),\n",
    "        # 将PIL Image或numpy.ndarray转换为tensor，并除255归一化到[0,1]之间\n",
    "        transforms.ToTensor(),\n",
    "        # 标准化处理-->转换为标准正太分布，使模型更容易收敛\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        image = Image.open(img_path)#打开图像\n",
    "        image = image_transform(image)\n",
    "        label = self.csv.iloc[idx, 1]#打开图像对应标签\n",
    "        label = np.array(label)#标签矩阵化\n",
    "        return image, label, #返回图像和标签\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv)#返回图片个数\n",
    "    \n",
    "    \n",
    "#数据集对象实例化\n",
    "mydata = MyData(root_dir = \"./after2\", csvfile = \"./after2/after2.csv\")#训练数据\n",
    "trainloader = torch.utils.data.DataLoader(mydata, batch_size = 50, shuffle = True)\n",
    "testdata = MyData(root_dir = \"./try\", csvfile = \"./try/try.csv\")#测试数据\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size = 1, shuffle = True)\n",
    "attackdata = MyData2(root_dir = \"./attack\", csvfile = \"./attack/attack.csv\")#攻击数据\n",
    "attackloader = torch.utils.data.DataLoader(attackdata, shuffle = False)\n",
    "afterdata = MyData2(root_dir = \"./train\", csvfile = \"./train/train.csv\")#扰动后数据\n",
    "afterloader = torch.utils.data.DataLoader(afterdata, batch_size = 1, shuffle = False)\n",
    "\n",
    "#神经网络\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)#第一卷积层\n",
    "        self.conv2 = nn.Conv2d(64, 1, 5)#第二卷积层\n",
    "        self.fc1 = nn.Linear(1600, 256)#第一全连接层\n",
    "        self.fc2 = nn.Linear(256, 3)\n",
    "        self.pool = nn.MaxPool2d(3, 3)#池化层\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))#第一层卷积后relu\n",
    "        x = self.pool(F.relu(self.conv2(x)))#第二层卷积后relu且池化\n",
    "        x = x.view(x.shape[0], -1)#将图片降维\n",
    "        x = F.relu(self.fc1(x))#带入第一全连接层\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "#神经网络对象实例化\n",
    "print(\"MODEL SET\")\n",
    "net = MyNet()\n",
    "\n",
    "#模型保存路径\n",
    "PATH = \".\\myModel-2.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b158d",
   "metadata": {},
   "source": [
    "# 训练神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac62f27c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!!!\n",
      "\n",
      "-->EPOCH: 0\n",
      "Loss 57.263757694512606\n",
      "\n",
      "-->EPOCH: 1\n",
      "Loss 11.539641374722123\n",
      "\n",
      "-->EPOCH: 2\n",
      "Loss 7.458524162881076\n",
      "\n",
      "-->EPOCH: 3\n",
      "Loss 2.812959139002487\n",
      "\n",
      "-->EPOCH: 4\n",
      "Loss 1.089401810138952\n",
      "\n",
      "-->EPOCH: 5\n",
      "Loss 0.5970718463213416\n",
      "\n",
      "-->EPOCH: 6\n",
      "Loss 0.49931786574597936\n",
      "\n",
      "-->EPOCH: 7\n",
      "Loss 0.38793960668408545\n",
      "\n",
      "-->EPOCH: 8\n",
      "Loss 0.3394934635289246\n",
      "\n",
      "-->EPOCH: 9\n",
      "Loss 0.304380609508371\n",
      "\n",
      "-->EPOCH: 10\n",
      "Loss 0.275157521093206\n",
      "\n",
      "-->EPOCH: 11\n",
      "Loss 0.2512229670483066\n",
      "\n",
      "-->EPOCH: 12\n",
      "Loss 0.22973737331631128\n",
      "\n",
      "-->EPOCH: 13\n",
      "Loss 0.21374396250212158\n",
      "\n",
      "-->EPOCH: 14\n",
      "Loss 0.20044026041614416\n",
      "\n",
      "Process done...\n",
      "Successfully Saved...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#定义损失和优化\n",
    "MyLoss = nn.CrossEntropyLoss()#交叉熵\n",
    "MyOptim = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)#更新权重使用SGD更新规则\n",
    "\n",
    "\n",
    "print(\"Start training!!!\")\n",
    "\n",
    "for epoch in range(15):#训练十次防止过拟合\n",
    "    runningLoss = 0\n",
    "    print(\"\\n-->EPOCH:\", epoch)\n",
    "    for i, data in enumerate(trainloader, 0):#遍历训练数据\n",
    "        \n",
    "        inputs, labels = data#带入训练数据\n",
    "        \n",
    "        MyOptim.zero_grad()#清零梯度\n",
    "        \n",
    "        #print(inputs.shape)\n",
    "        outputs = net(inputs.to(torch.float32))#带入神经网络\n",
    "        \n",
    "        \n",
    "        loss = MyLoss(outputs, labels)#计算损失\n",
    "        \n",
    "        \n",
    "        loss.backward()#反向传播\n",
    "        \n",
    "       \n",
    "        MyOptim.step()#更改权重\n",
    "        \n",
    "        runningLoss += loss.item()\n",
    "        \n",
    "    print(\"Loss\", runningLoss)\n",
    "    runningLoss = 0\n",
    "        \n",
    "  \n",
    "print(\"\\nProcess done...\")\n",
    "\n",
    "\n",
    "#保存神经网络模型\n",
    "\n",
    "torch.save(net.state_dict(), PATH)\n",
    "print(\"Successfully Saved...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ef829",
   "metadata": {},
   "source": [
    "# 测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9afbf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的是trainloader数据集\n",
      "共计： 1535 正确： 1020 准确率： 0.6644951140065146\n",
      "marin出现次数: 568 正确： 568 准确率： 1.0\n",
      "miku出现次数： 471 正确： 82 准确率： 0.1740976645435244\n",
      "kaguya出现次数: 496 正确： 370 准确率： 0.7459677419354839\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(PATH))\n",
    "net.eval()\n",
    "\n",
    "tc = [0,0,0]#统计各个人物出现的次数\n",
    "c = [0,0,0]#统计各个人物识别正确的次数\n",
    "#开始测试数据\n",
    "total_correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(afterloader):\n",
    "        images, labels = data\n",
    "        #print(images.shape)\n",
    "        outputs = net(images.to(torch.float32))\n",
    "        name, predicts = torch.max(outputs.data, 1)\n",
    "        if predicts == labels:\n",
    "            total_correct += 1\n",
    "            tc[labels] += 1\n",
    "        total += 1\n",
    "        c[labels] += 1\n",
    "print(\"当前使用的是trainloader数据集\")\n",
    "print(\"共计：\", total, \"正确：\", total_correct,\"准确率：\", total_correct / total)\n",
    "print(\"marin出现次数:\", c[0], \"正确：\", tc[0],\"准确率：\", tc[0] / c[0])\n",
    "print(\"miku出现次数：\", c[1], \"正确：\", tc[1],\"准确率：\", tc[1] / c[1])\n",
    "print(\"kaguya出现次数:\", c[2], \"正确：\", tc[2],\"准确率：\", tc[2] / c[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037a7ef",
   "metadata": {},
   "source": [
    "# 攻击网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d7cca72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinle\\AppData\\Local\\Temp\\ipykernel_43312\\2448173834.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ori = torch.tensor(ori)#转为tensor\n",
      "C:\\Users\\jinle\\AppData\\Local\\Temp\\ipykernel_43312\\2448173834.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 20 / 30 = 0.6666666666666666\n",
      "\n",
      "Epsilon: 0.01\tTest Accuracy = 16 / 30 = 0.5333333333333333\n",
      "\n",
      "Epsilon: 0.015\tTest Accuracy = 14 / 30 = 0.4666666666666667\n",
      "\n",
      "Epsilon: 0.02\tTest Accuracy = 11 / 30 = 0.36666666666666664\n",
      "\n",
      "Epsilon: 0.025\tTest Accuracy = 11 / 30 = 0.36666666666666664\n",
      "\n",
      "Epsilon: 0.03\tTest Accuracy = 11 / 30 = 0.36666666666666664\n",
      "\n",
      "Epsilon: 0.035\tTest Accuracy = 11 / 30 = 0.36666666666666664\n",
      "\n",
      "Epsilon: 0.04\tTest Accuracy = 11 / 30 = 0.36666666666666664\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(PATH))\n",
    "net.eval()\n",
    "\n",
    "\n",
    "#扰动\n",
    "epsilons = [0, .01, .015, .02, .025, .03, .035, .04]\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # 取图片张量每个元素的梯度\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # 在每个像素上添加扰动，幅度取决于epsilon\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # 将图片张量的每个元素的取值控制在0和1之间\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # 返回扰动图片\n",
    "    return perturbed_image\n",
    "\n",
    "def test( model, test_loader, epsilon):\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    examples = []\n",
    "    diffs = []\n",
    "    \n",
    "    #迭代梯度\n",
    "    for data, labels in test_loader:   \n",
    "        \n",
    "        inputs, labels = data, labels\n",
    "       \n",
    "        inputs.requires_grad = True\n",
    "        \n",
    "        output=net(inputs)#带入图片\n",
    "\n",
    "        init_pred = output.max(1, keepdim=True)[1] #开始预测\n",
    "\n",
    "        if init_pred.item() != labels.item():#如果预测错误的话则是本来就分类错的样本\n",
    "                continue#如果预测正确就是要进行扰动的样本\n",
    "\n",
    "        loss = F.nll_loss(output,labels)#计算损失\n",
    "\n",
    "        net.zero_grad()#清零模型梯度\n",
    "\n",
    "        loss.backward()#反向传播\n",
    "\n",
    "        data_grad = inputs.grad.data#获取反向传播的梯度\n",
    "\n",
    "        perturbed_inputs = fgsm_attack(inputs, epsilon, data_grad)#制作扰动图像\n",
    "\n",
    "        output = net(perturbed_inputs)#带入识别扰动图像\n",
    "\n",
    "        final_pred = output.max(1, keepdim=True)[1]#看扰动后图像的标签\n",
    "        \n",
    "        if final_pred.item() == labels.item():#如果添加扰动后依然识别成功\n",
    "            correct += 1\n",
    "        \n",
    "        #原本的图片\n",
    "        ori = inputs[0]\n",
    "        ori = torch.tensor(ori)#转为tensor\n",
    "        ori = ori.detach().numpy()#去除梯度\n",
    "        ori = np.transpose(ori, (1,2,0))#转置\n",
    "\n",
    "        #加了扰动后的图片\n",
    "        img = perturbed_inputs[0] # plt.imshow()只能接受3-d tensor，所以也要用image[0]消去batch那一维\n",
    "        img = torch.tensor(img)\n",
    "        img = img.detach().numpy() # floattensor转为ndarray\n",
    "        img = np.transpose(img, (1,2,0)) # 把channel那一维放到最后\n",
    "        \n",
    "        diff = 255*(ori-img)\n",
    "\n",
    "        examples.append(img)\n",
    "        diffs.append(diff)\n",
    "        \n",
    "    '''\n",
    "    plt.figure(figsize=(30,30))\n",
    "    \n",
    "    for i in range(10):\n",
    "        ax1 = plt.subplot(1, 10, i + 1) \n",
    "        ax1.axis(\"off\")       \n",
    "        ax1.imshow(examples[i])\n",
    "    \n",
    "    '''\n",
    "    plt.show()\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\\n\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc= test(net, attackloader, eps)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "\n",
    "plt.plot(epsilons, accuracies)\n",
    "plt.xlabel(\"epsilons\")\n",
    "plt.ylabel(\"accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee09fb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinle\\AppData\\Local\\Temp\\ipykernel_39268\\2809804108.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img)\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(PATH))\n",
    "net.eval()\n",
    "\n",
    "#扰动\n",
    "epsilons = [0, .01, .02, .03]\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # 取图片张量每个元素的梯度\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # 在每个像素上添加扰动，幅度取决于epsilon\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # 将图片张量的每个元素的取值控制在0和1之间\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # 返回扰动图片\n",
    "    return perturbed_image\n",
    "\n",
    "def test( model, test_loader, epsilon, i, csv_writer):\n",
    "    j = 1\n",
    "    \n",
    "    #迭代梯度\n",
    "    for data, labels in test_loader:   \n",
    "        \n",
    "        inputs, labels = data, labels\n",
    "       \n",
    "        inputs.requires_grad = True\n",
    "        \n",
    "        output=net(inputs)#带入图片\n",
    "\n",
    "        loss = F.nll_loss(output,labels)#计算损失\n",
    "\n",
    "        net.zero_grad()#清零模型梯度\n",
    "\n",
    "        loss.backward()#反向传播\n",
    "\n",
    "        data_grad = inputs.grad.data#获取反向传播的梯度\n",
    "\n",
    "        perturbed_inputs = fgsm_attack(inputs, epsilon, data_grad)#制作扰动图像\n",
    "\n",
    "        output = net(perturbed_inputs)#带入识别扰动图像\n",
    "\n",
    "        final_pred = output.max(1, keepdim=True)[1]#看扰动后图像的标签\n",
    "\n",
    "        #加了扰动后的图片\n",
    "        img = perturbed_inputs[0] # plt.imshow()只能接受3-d tensor，所以也要用image[0]消去batch那一维\n",
    "        img = torch.tensor(img)\n",
    "        img = img.detach().numpy() # floattensor转为ndarray\n",
    "        img = np.transpose(img, (1,2,0)) # 把channel那一维放到最后\n",
    "\n",
    "        save_name = \"%s%d-%d.jpg\" % (\"after2-\", i, j)\n",
    "        save_path = \"./after2/%s\" % (save_name)\n",
    "        Image.fromarray(np.uint8(img*255)).convert('RGB').save(save_path)\n",
    "        j += 1\n",
    "        csv_writer.writerow([save_name, labels.item()])\n",
    "\n",
    "\n",
    "i = 1\n",
    "# Run test for each epsilon\n",
    "csv_path = \"./after2/after2.csv\"\n",
    "file = open(csv_path, 'a+', encoding='utf-8', newline='')\n",
    "csv_writer = csv.writer(file)\n",
    "csv_writer.writerow([\"character\", \"name\"])\n",
    "for eps in epsilons:\n",
    "    test(net, afterloader, eps, i, csv_writer)\n",
    "    i += 1\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b8b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcf975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "无",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
